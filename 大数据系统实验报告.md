# 大数据系统实验报告

## 一、小组成员以及分工

冯筱璐（PB21151753）：主要负责Hadoop、Hbase环境配置，Hbase数据库搭建，以及Flask前端美化。

陆叶青（PB21081591）：主要负责编写爬虫程序，用于实现从科大候选网站中爬取相关文件，并以文件名和文件网址链接的形式组织文件的存储方式。

岳雨昕（）：

## 二、技术路线

### 1. 爬虫

从39个科大网站中爬取共659条数据。
### 2. Hadoop及Hbase环境搭建
#### Hadoop环境搭建
(1)**core-site.xml**
*默认文件系统url、临时文件存储目录*
![core-site](./pisc/core.png)
(2)**yarn-site.xml**
*nodemaneger辅助服务、resourcemanager的web应用、环境变量白名单*
![yarn-site](./pisc/yarn-site.png)
(3)**mapred-site.xml**
*mapper、reduce执行框架、应用程序类路径classpath*
![mapred-site](./pisc/mapred-site.png)
(4)**Hadoop-env.sh**
*JAVA_HOME配置*
![hadoop-env](./pisc/hadoop-env.png)
(5)**yarn-env.sh**
*JAVA_HOME配置*
![yarn-env](./pisc/yarn-env.png)
#### Hbase环境搭建
(1)**hbase-site.xml**
*hbase存储根目录z、ookeeper存储根目录、启动安全性检查、启动分布式模式*
![hbase-site](./pisc/hbase-site.png)
(2)**hdfs-site.xml**
*namenode存储目录、datanode存储目录、文件副本数量*
![hdfs-site](./pisc/hdfs-site.png)
(3)**hbase-env.sh**
*JAVA_HOME配置、启用内置ZK*
![hbase-env](./pisc/hbase-env-1.png)
#### 系统环境变量配置
**/etc/profile**
*JAVA_HOME、HADOOP_HOME、HBASE_HOME、HBASE_CONF_DIR
![profile](./pisc/profile.png)








## 三、功能介绍以及效果展示

### 1. 爬虫

笔者选取了总共39个科大网站，列举如下。其中{page_num}代表在实际爬取过程中要替换为index的部分。

| url网址                                                   | 网站名称                                       |
| --------------------------------------------------------- | ---------------------------------------------- |
| https://saids.ustc.edu.cn/15435/list{page_num}.htm        | 大数据学院                                     |
| https://finance.ustc.edu.cn/xzzx/list{page_num}.psp       | 财务处                                         |
| https://stuhome.ustc.edu.cn/2310/list{page_num}.htm       | 学工在线                                       |
| https://bwc.ustc.edu.cn/5655/list{page_num}.htm           | 保卫与校园管理处                               |
| http://zhb.ustc.edu.cn/18534/list{page_num}.htm           | 科技成果转移转化办公室                         |
| http://young.ustc.edu.cn/15056/list{page_num}.htm         | 青春科大                                       |
| http://cs.ustc.edu.cn/20158/list{page_num}.psp            | 计算机科学技术学院                             |
| http://cybersec.ustc.edu.cn/zlxz_23830/list{page_num}.psp | 网络空间安全学院                               |
| https://math.ustc.edu.cn/wswd/list{page_num}.htm          | 数学科学学院 类别：外事文件                    |
| https://math.ustc.edu.cn/tszy/list{page_num}.htm          | 数学科学学院 类别：图书资源                    |
| https://math.ustc.edu.cn/gkkzy/list{page_num}.htm         | 数学科学学院 类别：公开课资源                  |
| http://press.ustc.edu.cn/tgxz/list{page_num}.htm          | 出版社    类别：投稿须知                       |
| http://press.ustc.edu.cn/xtxxb/list{page_num}.htm         | 出版社    类别：选题信息表                     |
| http://press.ustc.edu.cn/bzgf/list{page_num}.htm          | 出版社    类别：标准规范                       |
| http://press.ustc.edu.cn/wjfg/list{page_num}.htm          | 出版社    类别：文件法规                       |
| http://press.ustc.edu.cn/jxzy/list{page_num}.htm          | 出版社    类别：教学资源                       |
| https://ispc.ustc.edu.cn/6299/list{page_num}.psp          | 信息科学试验中心   类别：申请与登记            |
| https://ispc.ustc.edu.cn/6298/list{page_num}.htm          | 信息科学试验中心   类别：技术文档              |
| http://ustcnet.ustc.edu.cn/33489/list{page_num}.psp       | 网络信息中心    类别：规章制度                 |
| http://ustcnet.ustc.edu.cn/33490/list{page_num}.psp       | 网络信息中心    类别：网字文件                 |
| http://ustcnet.ustc.edu.cn/33491/list{page_num}.psp       | 网络信息中心    类别：校网字文件               |
| http://ustcnet.ustc.edu.cn/33492/list{page_num}.psp       | 网络信息中心    类别：相关文件                 |
| https://zhc.ustc.edu.cn/zcgll/list{page_num}.htm          | 资产与后勤保障处    类别：资产管理类           |
| https://zhc.ustc.edu.cn/fcgll/list{page_num}.htm          | 资产与后勤保障处    类别：房产管理类           |
| https://zhc.ustc.edu.cn/cggll/list{page_num}.htm          | 资产与后勤保障处    类别：采购管理类           |
| https://zhc.ustc.edu.cn/hqbzyzhgll/list{page_num}.htm     | 资产与后勤保障处    类别：后勤保障与综合管理类 |
| https://sist.ustc.edu.cn/5111/list{page_num}.htm          | 信息科学技术学院    类别：本科生教育           |
| https://sist.ustc.edu.cn/5104/list{page_num}.htm          | 信息科学技术学院    类别：研究生教育           |
| https://sist.ustc.edu.cn/5128/list{page_num}.htm          | 信息科学技术学院    类别：党建工作             |
| https://sist.ustc.edu.cn/5095/list{page_num}.htm          | 信息科学技术学院    类别：学习工作             |
| https://sist.ustc.edu.cn/5085/list{page_num}.htm          | 信息科学技术学院    类别：科学研究             |
| https://sist.ustc.edu.cn/5079/list{page_num}.htm          | 信息科学技术学院    类别：信息服务             |
| https://sse.ustc.edu.cn/19878/list{page_num}.htm          | 软件学院      类别：教学管理                   |
| https://sse.ustc.edu.cn/19879/list{page_num}.htm          | 软件学院      类别：招生管理                   |
| https://sse.ustc.edu.cn/19880/list{page_num}.htm          | 软件学院      类别：就业管理                   |
| https://sse.ustc.edu.cn/19882/list{page_num}.htm          | 软件学院      类别：工程实践                   |
| https://sse.ustc.edu.cn/19884/list{page_num}.htm          | 软件学院      类别：党团建设                   |
| https://sse.ustc.edu.cn/19885/list{page_num}.htm          | 软件学院      类别：学生管理                   |

在实现爬虫的过程中，定义一个Crawler爬虫类来实现爬虫的功能。

首先对于一个网站的url，使用fetch_html()函数获取它的html文本。

![image-20241212143355675](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20241212143355675.png)

然后对一个web页面的html使用beautifulsoup进行解析，使用find_all（）方法得到此页面中所有文件标签列表并存储在soup_results里面。对于soup_results里面的每个文件再去find相应的title标签和url标签，值得注意的是直接从html解析出来的url标签需要在前面加上base_url才能形成完整的标签。

![image-20241212143608588](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20241212143608588.png)

对于soup_results中的每条内容，使用get_file_info()来得到相应的title和url。

![image-20241212144318133](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20241212144318133.png)

对于输入的文件格式，笔者使用json文件，存储需要爬取网站的内容。对于上述列表中提到的每个网站，通过查看网站相应的页面源代码，来获取其中文件内容的标签以及相应的爬取方式，并以字典的格式清晰地存储组织，示例如下：

![image-20241212144656316](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20241212144656316.png)

我们的系统从39个科大网站中爬取了659条文件数据，如下图所示。爬取的内容为文件名称和文件的url网址。

![image-20241212141453325](C:\Users\HP\AppData\Roaming\Typora\typora-user-images\image-20241212141453325.png)

### 2. Hbase数据库搭建
**(1)进程作用及协作模式：**
**Hadoop**:Namenode、Datanode
>**Namenode**：
>   - 作为 Hadoop 分布式文件系统（HDFS）的主节点，负责管理元数据（如文件的目录结构和块位置）和协调文件的存取。
>   - 在 HBase 中，HDFS 用于存储实际的表数据和 HBase 的 WAL（Write Ahead Log）。
>   - Namenode 确保 HBase 的数据可以可靠地分布和存储。

>**Datanode**：
>   - 作为 HDFS 的工作节点，负责存储数据块并响应客户端或 Namenode 的读写请求。
>   - HBase 中的数据（表数据和 WAL）分布存储在多个 Datanode 上。
>   - Datanode 会定期向 Namenode 发送心跳，确保数据块的健康状态。
---
**Hbase**:HMaster、HregionServer
>1. **HMaster**：
>   - HBase 的主控节点，负责分区的管理、分区分配、负载均衡和故障恢复。
>   - 在初始化时，HMaster 协调 Zookeeper 确定活跃的 HBase 集群。
>   - HMaster 确保所有 RegionServer 都正常运行，并根据需要重新分配 Region。

>2. **HRegionServer**：
>   - HBase 的工作节点，负责存储和管理表中的一个或多个 Region（表的分片）。
>   - 每个 HRegionServer 管理多个 HRegion，并执行读写请求。
>   - HRegionServer 将数据写入 HDFS 中的存储文件（HFile）和 WAL，以确保数据持久性。
---
**Zookeeper**:HQuorumPeer
>1. **HQuorumPeer**：
>   - Zookeeper 的节点，负责提供分布式协调服务，确保 HBase 集群的可靠性和高可用性。
>   - 维护 HBase 的元数据，比如 Region 的位置信息。
>   - 监控 HMaster 和 HRegionServer 的状态，确保主控节点和工作节点正常运行。
>     **具体功能**：
>   - 提供分布式锁机制，确保只有一个 HMaster 处于活跃状态。
>   - 通知 HMaster 或 HRegionServer 发生的事件（如节点下线、Region 变动等）。
---
**Thrift**:ThriftServer
>1. **ThriftServer**：
>   - 提供一个基于 Thrift 协议的接口，使得外部应用（如 Python、Ruby 等非 Java 客户端）可以访问 HBase 数据。
>   - 它通过调用 HBase API 直接与 HMaster 和 HRegionServer 交互，完成数据的读写操作。
>   - ThriftServer 提供了一种语言无关的 RPC 框架，可以方便地扩展 HBase 的访问方式。


**(2)进程协作流程**
1. **初始化**：
   - HMaster 启动时，通过 Zookeeper 注册自身并确认自身为活跃的主控节点。
   - HRegionServer 启动时，向 Zookeeper 注册并报告其状态。
   - Zookeeper 保持 HMaster 和 HRegionServer 的心跳连接，确保集群的稳定运行。

2. **数据存储**：
   - 客户端通过 ThriftServer 或直接通过 Java API 向 HBase 发起数据写入请求。
   - 请求被转发到 HRegionServer，数据首先写入 WAL（存储在 HDFS 上）以确保持久性，然后写入内存（MemStore）。
   - 当 MemStore 达到一定大小时，数据被刷写到 HDFS 的 HFile 中。

3. **数据读取**：
   - 客户端请求读取数据时，通过 Zookeeper 查询元数据，定位相应的 HRegionServer。
   - 请求被路由到存储目标数据的 HRegionServer，数据从 MemStore 或 HFile 中读取并返回。

4. **节点失效处理**：
   - 如果 HRegionServer 宕机，Zookeeper 通知 HMaster。
   - HMaster 通过重新分配失效节点上的 Region，恢复集群的正常运行。

**进程启动后展示：**
![jps](./pisc/jps.png)

**(2)爬虫数据上传：**
以文件名'title'为'row_key'，在Hbase中建表上传爬取的文件数据
![import](./pisc/import.png)

**上传成功效果展示：**
![hbase](./pisc/hbase2.png)

## 四、遇到的问题及解决
### 1. hadoop与hbase的jar包不匹配：
阻止Hbase动态地将 Hadoop 的 Classpath 加载到 HBase 的运行时环境中，而是依赖手动设置的 Classpath。
![pro1](./pisc/hbase-env-2.png)
### 2. HBase 无法访问 HDFS 文件系统中的数据存储目录：
fs.defaultFS与hbase.rootdir需要配置一致。
![pro2_hadoop](./pisc/core.png)
![pro2_hbase](./pisc/hbase-site.png)



## 五、实践心得

冯筱璐：在配置环境的过程中，进一步熟悉了linux的命令行操作，理解了Hadoop、Hbase的配置文件中各个参数的含义，从更为底层的视角理解了Hadoop、Hbase与Zookeeper的协同工作逻辑，更好地掌握了大数据系统的相关知识。

陆叶青：在本次实验中，我学习了如何使用爬虫来爬取网页内容，通过beautifulsoup这一库函数来实现html标签的查找。在设计输入时，使用json文件定义输入网站的url和标签内容，使得针对不同网页，可以有统一的爬取方式，方便复用与迁移。

岳雨昕：